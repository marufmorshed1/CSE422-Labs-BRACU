# -*- coding: utf-8 -*-
"""3_20101239_MdRaihanulIslamBhuiyan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1594V7PEsh_Fp-InCilcSQeqk9q1D2uli
"""

import pandas as pd
import numpy as np
import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

sample=pd.read_csv('/content/wine.csv')
sample.shape

sample

sample.isnull().sum()

sample.info()

from sklearn.preprocessing import LabelEncoder


#creating object
enc=LabelEncoder()

#Encoding quality column

sample['quality']=enc.fit_transform(sample['quality'])

#comparing two column

print( sample[['quality']].head(100))

sample_copy=sample.copy()

"""Minimax Formula"""

for column in sample_copy.columns:
  sample_copy[column] = (sample_copy[column] - sample_copy[column].min()) / (sample_copy[column].max() - sample_copy[column].min())    

sample_copy

sample.columns
X= sample_copy[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol']]
y= sample_copy[['quality']]

sample_corr=sample_copy.corr()
sample_corr

import seaborn as sns

sns.heatmap(sample_corr,cmap='YlGnBu')

drop=['citric acid','alcohol']
for i in drop:
  sample_copy=sample_copy.drop(i,axis=1)

sample_copy



X = sample_copy.iloc[:, :-1]
y = sample_copy.iloc[:, -1]

from sklearn.model_selection import train_test_split
sample_copy = sample_copy.drop('quality',axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,  random_state=1,test_size=0.2)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Logistic Regression"""

log=LogisticRegression()

log.fit(X_train, y_train)

"""Predict from logistic Regression"""

predict=log.predict(X_test)
predict

log_accuracy=accuracy_score(y_test, predict)

log_accuracy

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
classification = DecisionTreeClassifier(criterion='entropy',random_state=1)
classification.fit(X_train,y_train)

predict = classification.predict(X_test)
decision_accuracy=accuracy_score(predict, y_test)
decision_accuracy

import matplotlib.pyplot as plt
figure= plt.figure()
columns = ['Logistic regression','Decision tree']
accuracy = [log_accuracy, decision_accuracy]

plt.bar(columns, accuracy, width=0.3)